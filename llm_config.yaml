# CrewAI LLM Configuration for Ollama
llm:
  model: "ollama/hf.co/unsloth/medgemma-4b-it-GGUF:Q4_K_M"
  base_url: "http://localhost:11434"
  temperature: 0.7
  max_tokens: 2000

# Ollama specific configuration
ollama:
  api_base: "http://localhost:11434"
  model: "hf.co/unsloth/medgemma-4b-it-GGUF:Q4_K_M"
